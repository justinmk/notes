TODO:
    https://arstechnica.com/information-technology/2017/03/802-eleventy-what-a-deep-dive-into-why-wi-fi-kind-of-sucks/
    TCP Puzzlers https://news.ycombinator.com/item?id=12315814
    Rapid DHCP: Or, how do Macs get on the network so fast? http://cafbit.com/entry/rapid_dhcp_or_how_do

TOOLS
================================================================================
wireshark
    UI:
        Follow roundtrips/streams: right-click a packet, select "Follow > HTTP Stream"

    Analyze remote traffic in wireshark
        ssh -t user@host 'mkfifo traffic ; sudo tcpdump -U -s0 "not port 22" -i any -w traffic'
        ssh user@host 'cat traffic' | wireshark -k -i -

    Filters  https://gitlab.com/wireshark/wireshark/-/wikis/DisplayFilters
        (frame contains "mozilla")    search for “mozilla” anywhere in the packet
        (tcp.port == 443)             tcp port
        (dns.resp.len > 0)            all DNS responses
        (ip.addr == 52.7.23.87)       source or dest IP address
        tls and (ip.dst_host matches "50")

    TLS (SSL/HTTPS)
        https://gitlab.com/wireshark/wireshark/-/wikis/TLS
        Capturing Node.js traffic:
            https://gist.github.com/dfrankland/0fec2cd565f1f7b78fb0e3ededf36b89
            1. Install "sslkeylog" package, it sets global hooks in the `https`
               module to capture SSL keys.
               npm i -D sslkeylog
            2. Activate sslkeylog in your application:
               import sslkeylog from 'sslkeylog';
               sslkeylog.hookAll();
            3. Start your application with SSLKEYLOGFILE set:
               SSLKEYLOGFILE=~/foo.log node app.js
            4. In Wireshark, input the file path in Preferences > Protocols > TLS.

    https://jvns.ca/blog/2018/06/19/what-i-use-wireshark-for/
    http://jvns.ca/blog/2016/03/16/tcpdump-is-amazing/
        tcpdump is low-level, whereas wireshark/tshark _understands_ protocols (HTTP, even Monogo db...).
        tcpdump -i any
            13:21:15.025449 IP 10.0.2.2.57434 > 10.0.2.15.ssh: Flags [.], ack 8911568, win 65535, length 0
            13:21:15.025936 IP 10.0.2.15.ssh > 10.0.2.2.57434: Flags [.], seq 8911568:8913028, ack 6425, win 52560, length 1460
        tshark -i any
            19858  76.771803    10.0.2.15 -> 10.0.2.2     SSH 1516 Encrypted response packet len=1460
            19859  76.772656    10.0.2.15 -> 10.0.2.2     SSH 560 Encrypted response packet len=504
            19860  76.772870     10.0.2.2 -> 10.0.2.15    TCP 62 57434 > ssh [ACK] Seq=10465 Ack=12908205 Win=65535 Len=0

https://mitmproxy.org/
> interactive console program that allows traffic flows to be intercepted,
> inspected, modified and replayed.

list TCP, UDP ports on localhost ("nmap is not the best tool for this": https://superuser.com/a/1265254):
    ss -tunpo state listening

TCP performance profiling: https://github.com/fastos/tcpdive


SSH "application-level port forwarding" / SOCKS5
================================================================================
https://www.chromium.org/developers/design-documents/network-stack/socks-proxy

Creates an auto-closing SSH Tunnel (will close if Chrome exits) to a remote ssh
server bound to localhost:7070 socks 5 proxy.

    ssh -C -f -q -D 7070 user@server sleep 10
    open -a '/Applications/Google Chrome.app' --args \
        --proxy-server="socks5://localhost:7070" \
        --host-resolver-rules="MAP * 0.0.0.0 , EXCLUDE myproxy"

The -f option backgrounds ssh and the remote command `sleep 10` is specified
to allow some time to start the service which is to be tunnelled.  If no
connections are made during that time, ssh will exit.

SSH simple port forwarding
================================================================================
    ssh -p 22 -N -L localhost:8082:localhost:8082 user@server


DNS
================================================================================
How DNS works: hierarchy of servers.

    $ dig +trace sink.io

    ; <<>> DiG 9.10.6 <<>> +trace sink.io
    ;; global options: +cmd
    .                       84993   IN      NS      g.root-servers.net.
    …
    .                       84993   IN      NS      b.root-servers.net.
    ;; Received 540 bytes from 192.168.2.1#53(192.168.2.1) in 44 ms
    io.                     172800  IN      NS      ns-a1.io.
    io.                     172800  IN      NS      ns-a3.io.
    …
    ;; Received 803 bytes from 199.9.14.201#53(b.root-servers.net) in 198 ms
    sink.io.                86400   IN      NS      ns1hnx.name.com.
    sink.io.                86400   IN      NS      ns2kry.name.com.
    …
    ;; Received 647 bytes from 74.116.178.1#53(ns-a3.io) in 27 ms
    sink.io.                300     IN      CNAME   justinmk.github.io.
    ;; Received 66 bytes from 162.88.60.47#53(ns2kry.name.com) in 17 ms

    $ dig +trace sink.io
                                  # Q: A-record for sink.io?
        b.root-servers.net        # A: NS servers for the io. TLD
                                  # Q: A-record for sink.io?
          ns-a3.io                # A: NS servers for the sink.io. zone
                                  # Q: A-record for sink.io?
            ns2kry.name.com       # A: CNAME


UDP
================================================================================
TCP is optimized for accurate rather than timely delivery and can incur
relatively long delays (~seconds) while waiting for out-of-order messages or
re-transmissions. Thus real-time applications such as VoIP use UDP instead.


TCP/IP
================================================================================
IP _delivers_ the data, TCP _tracks_ the data (segments).

TCP accepts data from a stream, divides it into chunks, adds header.
After packaging, the segment is "encapsulated" into an IP datagram, and exchanged with peers.

Because TCP packets do NOT include a session identifier, both endpoints identify
the session by client address + port.

IP packet
    Almost every packet (with some exceptions like ARP packets) has an IP header.
    14 fields in the IP header. 3 important ones:
      1. source IP address
      2. destination IP address
      3. TTL (hop limit): decremented by 1 at each hop (prevent infinite loop in case of routing error)
      4. ‘protocol’ field tells you the protocol (TCP/UDP/…).
    port isn’t in the IP header! That's at the TCP/UDP protocol layer. (and is why
    TCP port 8080 and UDP port 8080 are different ports, and can run different
    services!)

subnet
    CIDR notation – 168.23.0.0/8 means “all the packets that have the same
    first 8 bits as the packet 168.23.0.0”. So 168.*.*.*
    8 = # of leading ones in network mask => 255.0.0.0

loopback  https://blog.cloudflare.com/this-is-strictly-a-violation-of-the-tcp-specification/
    - loopback has no buffering, congestion, or packet loss (unless the
      listening application does not call accept() fast enough)
    - loopback works magically: when an application sends packets to it, they
      immediately get delivered within the send() syscall
    - calling send() over loopback triggers iptables, network stack delivery
      mechanisms and delivers the packet to the appropriate queue of the target
      application.

TCP PDU: segment = (TCP header + data)
    header: 10 mandatory fields + optional extension field
    data:   length NOT specified in the TCP header! Calculated by subtracting (TCP
            header + encapsulating IP header) from the total IP datagram length (specified
            in the IP header).

TCP header fields:
    Source port (16 bits)
    Destination port (16 bits)
    Sequence number (32 bits)
    Acknowledgment number (32 bits): (+ACK flag)
    Data offset (4 bits): TCP header size in 32-bit words (min 5, max 15 => 20-60 bytes)
    Reserved (3 bits)
    Flags (9 bits) (aka Control bits)
      URG (1 bit): enables the Urgent field
      ACK (1 bit): enables the Acknowledgment field. All packets after the initial SYN packet sent by the client should have this flag set.
      PSH (1 bit): Push function. Asks to push the buffered data to the receiving application.
      RST (1 bit): Reset the connection
      SYN (1 bit): Synchronize sequence numbers. Only the first packet from each end should have this flag set.
      FIN (1 bit): Last packet from sender.
      ...
    Window size (16 bits): receive-window size, for Flow control and Window Scaling.
    Checksum (16 bits)
    Urgent pointer (16 bits): (+URG flag) indicates the last urgent data byte.
    Options (Variable 0–320 bits)
    Padding: zero-padding to ensure TCP header ends at 32-bit boundary.

TCP phases: connection establishment, data transfer, connection termination

TCP states:
    LISTEN (server) waiting for a connection request
    SYN-SENT (client) waiting for a matching connection request after having sent one.
    SYN-RECEIVED (server) waiting for a confirming connection request ACK after having both received and sent.
    ESTABLISHED (both) open connection, data received can be delivered to the user. The normal state for the data transfer phase of the connection.
    FIN-WAIT-1 (both) waiting for termination request from remote TCP, or ACK of the termination request previously sent.
    FIN-WAIT-2 (both) waiting for termination request from remote TCP.
    CLOSE-WAIT (both) waiting for termination request (close()) from local user (application). Socket can be in CLOSE_WAIT indefinitely until the application calls close()!
    CLOSING (both) waiting for a connection termination request acknowledgment from the remote TCP.
    LAST-ACK (both) waiting for ACK of termination request previously sent
    TIME-WAIT (either) waiting for enough time to pass to be sure the remote TCP received ACK of termination request.
    CLOSED (both) no connection state.


https://morsmachine.dk/tcp-consensus
> TCP connection state is defined largely by per-implementation heuristics.
> A box in the middle of the network can guess the state of a TCP connection
  only by guessing the meaning of a sequence of TCP/IP packets.


https://www.destroyallsoftware.com/compendium/network-protocols?share_key=97d3ba4c24d21147
    The network stack does several seemingly-impossible things:
      - reliable transmission over our unreliable networks
      - adapts smoothly to network congestion
      - provides addressing to billions of active nodes
      - routes packets around damaged network infrastructure, reassembling them
        in the correct order on the other side
      - accommodates esoteric analog hardware needs, like balancing the charge
        on the two ends of an Ethernet cable

    Network routing

    We can't provide a direct, uninterruptible path between each machine.
    Instead, data is bucket-brigaded: handed off from one router to the next,
    maintaining a crude routing table showing which routers are closer.

    Routing is two sub-problems:
      addressing. handled by IP
      routing.
        Cisco ASR 9922 routers have capacity of 160 terabits per second. Assuming full 1500 byte packets (12000 bits), that's 13333333333 packets per second in a single 19 inch rack!
        BGP communicates updates route tables between different routers.

    Packet switching
    What happens when the data is large? What if we request the 88.5 MB video of The Birth & Death of JavaScript?

    We could try to design a network where the 88.5 MB document is sent from the web server to the first router, then to the second, and so on. Unfortunately, that network wouldn't work at Internet scale, or even at intranet scale.

    Instead, we break them down into packets, usually in the neighborhood of 1400 bytes each.
    Our video file will be broken into 63214 or so separate packets for transmission.

    Out-of-order packets
    TCP packet reassembly is done using the simplest imaginable mechanism:
    a counter. Each packet is assigned a sequence number when it's sent. On the
    receiving side, the packets are put in order by sequence number.

    How do we know when the file is finished, though? TCP doesn't say anything
    about that; it's the job of higher-level protocols. For example, HTTP
    responses contain a "Content-Length" header specifying the response length
    in bytes. The client reads the Content-Length, then keeps reading TCP
    packets, assembling them back into their original order, until it has all of
    the bytes specified by Content-Length. This is one reason that HTTP headers
    (and most other protocols' headers) come before the response payload:
    otherwise, we wouldn't know the payload's size.

    When we say "the client" here, we're really talking about the entire
    receiving computer. TCP reassembly happens inside the kernel, so
    applications like web browsers and curl and wget don't have to manually
    reassemble TCP packets. But the kernel doesn't handle HTTP, so applications
    do have to understand the Content-Length header and know how many bytes to
    read.

    With sequence numbers and packet reordering, we can transmit large sequences of bytes even if the packets arrive out-of-order. But what if a packet is lost in transit, leaving a hole in the HTTP response?

    Transmission windows and slow start
    I did a normal download of The Birth & Death of JavaScript with Wireshark
    turned on. Scrolling through the capture, I see packet after packet being
    received successfully.

    For example, a packet with sequence number 563321 arrived. Like all TCP
    packets, it had a "next sequence number", which is the number used for the
    following packet. This packet's "next sequence number" was 564753. The next
    packet did, in fact, have sequence number 564753 so everything was good.
    This happens thousands of times per second once the connection gets up to
    speed.

    Occasionally, my computer sends a message to the server saying, for example,
    "I've received all packets up to and including packet number 564753." That's
    an ACK. On a new connection, the Linux kernel sends an ACK after every ten
    packets. This is controlled by the TCP_INIT_CWND constant (CWND = congestion
    window: the amount of data allowed in flight at once. If the network becomes
    congested (overloaded), the window size will be reduced, slowing packet
    transmission.).

    Ten packets is about 14 KB, so we're limited to 14 KB of data in flight at
    a time. This is part of TCP slow start: connections begin with small
    congestion windows. If no packets are lost, the receiver will continually
    increase the congestion window, allowing more packets in flight at once.

    Eventually, a packet will be lost, so the receive window will be decreased,
    slowing transmission. By automatically adjusting the congestion window, as
    well as some other parameters, the sender and receiver keep data moving as
    quickly as the network will allow, but no quicker.

    This happens on both sides of the connection: each side ACKs the other
    side's messages, and each side maintains its own congestion window.
    Asymmetric windows allow the protocol to take full advantage of network
    connections with asymmetric upstream and downstream bandwidth, like most
    residential and mobile Internet connections.

    Reliable transmission
    Computers are unreliable; networks made of computers are extra unreliable.
    Failure is a normal part of operation and must be accommodated. In a packet
    network, this means retransmission: if the client receives packets number
    1 and 3 but doesn't receive 2 then it needs to ask the server to re-send.

    When receiving thousands of packets per second, as in our 88.5 MB video
    download, mistakes are almost guaranteed. To demonstrate that, let's return
    to my Wireshark capture of the download. For thousands of packets,
    everything goes normally. Each packet specifies a "next sequence number",
    followed by another packet with that number.

    Suddenly, something goes wrong. The 6269th packet has a "next sequence
    number" of 7208745 but that packet never comes. Instead, a packet with
    sequence number 7211609 arrives. This is an out-of-order packet: something
    is missing.

    We can't tell exactly what went wrong here. Maybe one of the intermediate
    routers on the Internet was overloaded. Maybe my local router was
    overloaded. Maybe someone turned a microwave on, introducing electromagnetic
    interference and slowing my wireless connection. In any case, the packet was
    lost and the only indication is the unexpected packet.

    TCP has no special "I lost a packet!" message. Instead, ACKs are cleverly
    reused to indicate loss. Any out-of-order packet causes the receiver to
    re-ACK the last "good" packet – the last one in the correct order. In
    effect, the receiver is saying "I received packet 5 which I'm ACKing. I also
    received something after that, but I know it wasn't packet 6 because it
    didn't match the next sequence number in packet 5."

    If two packets simply got switched in transit, this will result in a single
    extra ACK and everything will continue normally after the out-of-order
    packet is received. But if the packet was truly lost, unexpected packets
    will continue to arrive and the receiver will continue to send duplicate
    ACKs of the last good packet. This can result in hundreds of duplicate ACKs.

    When the sender sees three duplicate ACKs in a row, it assumes that the
    following packet was lost and retransmits it. This is called TCP fast
    retransmit because it's faster than the older, timeout-based approach. It's
    interesting to note that the protocol itself doesn't have any explicit way
    to say "please retransmit this immediately!" Instead, multiple ACKs arising
    naturally from the protocol serve as the trigger.

    (An interesting thought experiment: what happens if some of the duplicate ACKs are lost, never reaching the sender?)

    Retransmission is common even in networks working normally. In a capture of our 88.5 MB video download, I saw this:
      Congestion window quickly increases to ~1 megabyte due to continuing successful transmission.
      A few thousand packets show up in order; everything is normal.
      One packet comes out of order.
      Data continues pouring in at megabytes per second, but the packet is still missing.
      My machine sends dozens of duplicate ACKs of the last known-good packet, but the kernel also stores the pending out-of-order packets for later reassembly.
      The server receives the duplicate ACKs and resends the missing packet.
      My client ACKs both the previously-missing packet and the later ones that were already received due to out-of-order transmission. This is done by simply ACKing the most recent packet, which implicitly ACKs all earlier ones as well.
      The transfer continues, but with a reduced congestion window due to the lost packet.
      This is normal; it's happened in every capture of the full download that I've done. TCP is so successful at its job that we don't even think of networks as being unreliable in our daily use, even though they fail routinely under normal conditions.

    Physical networking
    All of this network data has to be transferred over physical media like
    copper, fiber optics, and wireless radio. Of the physical layer protocols,
    Ethernet is the most well known. Its popularity in the early days of the
    Internet led us to design other protocols to accommodate its limitations.

    As a protocol, ethernet has two primary jobs:
    1. each device needs to notice that it's connected to something, and some
       parameters like connection speed need to be negotiated.
    2. After link is established, carry data.

    A full Ethernet packet contains:

      - The preamble, which is 56 bits (7 bytes) of alternating 1s and 0s. The
        devices use this to synchronize their clocks, sort of like when people
        count off "1-2-3-GO!" Computers can't count past 1 so they synchronize
        by saying "10101010101010101010101010101010101010101010101010101010".
      - An 8-bit (1 byte) start-frame delimiter, which is the number 171
        (10101011 in binary). This marks the end of the preamble. Notice that
        it's "10" repeated again, until the end where there's a "11".
      - The frame itself, which contains the source and destination addresses,
        the payload, etc., as described above.
      - An interpacket gap of 96 bits (12 bytes) where the line is left idle.

    Networking meets the real world
    Digital systems don't exist; everything is analog.

    Suppose we have a 5-volt CMOS system. (CMOS is a type of digital system; don't worry about it if you're not familiar.) This means that a fully-on signal will be 5 volts, and a fully-off signal will be 0. But nothing is ever fully on or fully off; the physical world doesn't work like that. In reality, our 5-volt CMOS system will consider anything above 1.67 volts to be a 1 and anything below 1.67 to be 0.

    (1.67 is 1/3 of 5. Let's not worry about why the threshold is 1/3. If you want to dig, there's a wikipedia article, of course! Also, Ethernet isn't CMOS or even related to CMOS, but CMOS and its 1/3 cutoff make for a simple illustration.)

    Our Ethernet packets have to go over a physical wire, which means changing the voltage across the wire. Ethernet is a 5-volt system, so we naively expect each 1 bit in the Ethernet protocol to be 5 volts and each 0 bit to be 0 volts. But there are two wrinkles: first, the voltage range is -2.5 V to +2.5 V. Second, and more strangely, each set of 8 bits gets expanded into 10 bits before hitting the wire.

    There are 256 possible 8-bit values and 1024 possible 10-bit values, so imagine this as a table mapping them. Each 8-bit byte can be mapped to any of four different 10-bit patterns, each of which will be turned back into the same 8-bit byte on the receiving end. For example, the 10-bit value 00.0000.0000 might map to the 8-bit value 0000.0000. But maybe the 10-bit value 10.1010.1010 also maps to 0000.0000. When an Ethernet device sees either 00.0000.0000 or 10.1010.1010 they'll be understood as the byte 0 (binary 0000.0000).

    (Warning: there are going to be some electronics words now.)

    This exists to serve an extremely analog need: balancing the voltage in the devices. Suppose this 8-bit-to-10-bit encoding doesn't exist, and we send some data that happens to be all 1s. Ethernet's voltage range is -2.5 to +2.5 volts, so we're holding the Ethernet cable's voltage at +2.5 V, continually pulling electrons from the other side.

    Why do we care about one side pulling more electrons than the other? Because the analog world is a mess and it will cause all kinds of undesirable effects. To take one: it can charge the capacitors used in low-pass filters, creating an offset in the signal level itself, eventually causing bit errors. Those errors would take time to accumulate, but we don't want our network devices to suddenly corrupt data after two years of uptime simply because we happened to send more binary 1s than 0s.

    (Electronics words end here.)

    By using an 8b/10b encoding, Ethernet can balance the number of 0s and 1s sent over the wire, even if we send data that's mostly 1s or mostly 0s. The hardware tracks the ratio of 0s to 1s, mapping outgoing 8-bit bytes to different options from the 10-bit table to achieve electrical balance. (Newer Ethernet standards, like 10 GB Ethernet, use different and more complex encoding systems.)

    We'll stop here, because we're already beyond the scope of what can be considered programming, but there are many more protocol issues to accommodate the physical layer. In many cases, the solutions to hardware problems lie in the software itself, as in the case of the 8b/10b coding used to correct DC offset. This is perhaps a bit disconcerting to us as programmers: we like to pretend that our software lives in a perfect Platonic world, devoid of the vulgar imperfections of physicality. In reality, everything is analog, and accommodating that complexity is everyone's job, including the software's.

    The interconnected network stack
    - Internet protocols are best thought of as a stack of layers.
    - Ethernet provides physical data transfer and link between two point-to-point devices.
    - IP provides a layer of addressing, allowing routers and large-scale networks to exist, but it's connectionless.
    - Packets are fired into the ether, with no indication of whether they arrived or not.
    - TCP adds a layer of reliable transmission by using sequence numbers, acknowledgement, and retransmission.
    - Application-level protocols like HTTP are layered on top of TCP.

    The independence of these layers is important. For example, when packets
    were lost during my 88.5 MB video transfer, the Internet's backbone routers
    didn't know; only my machine and the web server knew. Dozens of duplicate
    ACKs from my computer were all dutifully routed over the same routing
    infrastructure that lost the original packet. It's possible that the router
    responsible for dropping the lost packet was also the router carrying its
    replacement milliseconds later. This is an important point for understanding
    the Internet: the routing infrastructure doesn't know about TCP; it only
    routes. (There are exceptions to this, as always, but it's generally true.)

    Layers of the protocol stack operate independently, but they weren't designed independently. Higher-level protocols tend to be built on lower-level ones: HTTP is built on TCP is built on IP is built on Ethernet. Design decisions in lower levels often influence decisions in higher levels, even decades later.

    Ethernet is old and concerns the physical layer, so its needs set the base parameters. An Ethernet payload is at most 1500 bytes.

    The IP packet needs to fit within an Ethernet frame. IP has a minimum header size of 20 bytes, so the maximum payload of an IP packet is 1500 - 20 = 1480 bytes.

    Likewise, the TCP packet needs to fit within the IP packet. TCP also has a minimum header size of 20 bytes, leaving a maximum TCP payload of 1480 - 20 = 1460 bytes. In practice, other headers and protocols can cause further reductions. 1400 is a conservative TCP payload size.

    The 1400 byte limit influences modern protocols' designs. For example, HTTP
    requests are generally small. If we fit them into one packet instead of two,
    we reduce the probability of losing part of the request, with
    a correspondingly reduced likelihood of TCP retransmissions. To squeeze
    every byte out of small requests, HTTP/2 specifies compression for headers,
    which are usually small.

    HTTP/2 does header compression to meet the constraints of TCP, which come
    from constraints in IP, which come from constraints in Ethernet, which was
    developed in the 1970s, introduced commercially in 1980 and standardized in
    1983.

    One final question: why is the Ethernet payload size set at 1500 bytes?
    There's no deep reason; it's just a nice trade-off point. There are 42 bytes
    of non-payload data needed for each frame. If the payload maximum were only
    100 bytes, only 70% (100/142) of time would be spent sending payload.
    A payload of 1500 bytes means about 97% (1500/1542) of time is spent sending
    payload, which is a nice level of efficiency. Pushing the packet size higher
    would require larger buffers in the devices, which we can't justify simply
    to get another percent or two of efficiency. In short: HTTP/2 has header
    compression because of the RAM limitations of networking devices in the late
    1970s.


OSI model
================================================================================
At each level N, two entities at the communicating devices (layer N peers)
exchange protocol data units (PDUs) by means of a layer N protocol. Each PDU
contains a payload (service data unit = SDU) plus headers/footers.

- Layer 1: Physical Layer [baud = PPS] [ex: RS-232, RJ45]
  xmit unstructured raw data between a device and a physical transmission medium.
  Converts digital bits into electrical/radio/optical signals.
  timing and frequency for wireless devices.

- Layer 2: Data Link Layer [frame] [ex: Ethernet]
  NODE-TO-NODE (not network!) data transfer: link between two directly connected nodes.
  Frame delimiting and recognition; handling address space; transparent data
  transfer of LLC PDUs; CRC Checking; control of access to the physical
  transmission medium, like Token passing and CSMA/CD

  sublayer 1: MAC control (Medium access): control how devices in a network gain access/permission to transmit data.
  sublayer 2: LLC (Logical link control)

- Layer 3: Network Layer [packet] [ex: IP, ICMP]
  Multi-node NETWORK: nodes can xmit merely providing a message + address and letting the network deliver it.
  Transfer variable-length data sequences (packets).

- Layer 4: Transport Layer [segment, datagram] [ex: TCP, UDP]
  RELIABLE (QoS) transfer of data between session-entities (ACK, track segments, re-transmit failed)

- Layer 7: application
