TODO:
    https://course.fast.ai/
    https://developers.google.com/machine-learning/crash-course/
    https://mlu-explain.github.io/

GPT = generative pretrained transformer
    - "Transformer": detects subtle relationships even amongst distant data elements in a sequence.
        - marketing name for "attention net"
        - "self-attention": differentially weighting each part of the input (which includes the recursive output)
        - from 2017 Google paper
          https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html
          https://arxiv.org/abs/1706.03762
    - trained by RLHF
      https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback

https://news.ycombinator.com/item?id=14485362
    > You don’t need Google-scale data to use deep learning. Using all of the above
    > means that even your average person with only a 100-1000 samples can see some
    > benefit from deep learning. With all of these techniques you can mitigate the
    > variance issue, while still benefitting from the flexibility.
    |
    └─  _Transfer Learning_
        "Easiest way to use deep learning without a lot of data: download
        a pretrained model and fine-tune the last few layers on your small
        dataset. In many domains (like image classification) fine-tuning works
        extremely well, because the pretrained model has learned generic
        features in the early layers that are useful for many datasets, not
        just the one trained on. Even the best skin cancer classifier
        (http://www.nature.com/articles/nature21056) was pretrained on ImageNet.
        |
        └─ "This is how the great http://course.fast.ai course begins - download VGG16,
        |   finetune the top layer with a single dense layer, get amazing results. The
        |   second or third class shows how to make the top layers a bit more complex to get
        |   even better accuracy."
        |
        └─ Remove the last "layer" of the neural net. Then you can use
           a classifier (e.g. SVM) to add the layer back, for your specific
           application. E.g. with deep learning, the NN finds features, these
           are in the last "hidden layer". That layer is generalized, you can
           use those features to train for your specific application.

