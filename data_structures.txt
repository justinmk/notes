♡ DICTIONARY OF ALGORITHMS & DATA STRUCTURES: https://xlinux.nist.gov/dads/

---

A data structure is "naturally recursive" if it can be cut in half and both
parts treated as the same type of structure. (trees, arrays, stacks, ...).

Stacks and Queues have equivalent _average_ waiting time. Queue is important
when _order_ of processing (or maximum waiting time) is important.

vector vs linked-list:
    linked-list is ~never the right choice (unless your workload is dominated by splitting/merging costs).
    linked-list traversal = scattered random memory-access (no spatial locality) => defeats caches => 10x slower

lock-free concurrency
    C: http://concurrencykit.org/
    mscorlib: ConcurrentQueue.cs
        - all public/protected methods are thread-safe
        - use CAS; "SpinWait" for backoff
        - GetEnumerator(), ToArray(), ToList() need to take "snapshot".
        - ConcurrentQueue is a linked list of small arrays, each node is called a
          segment. A segment contains an array, a pointer to the next segment,
          and m_low, m_high indices recording the first and last valid elements
          of the array.
    other notes:
        copy shared state to local vars before doing work, then use CAS
        C#: struct assignement/copying is not atomic
        C#: lock() is syntax-sugar for try-finally Monitor.Enter/Monitor.Exit.
        event queue is an alternative to polling

ring buffer / circular buffer / bounded queue / FIFO
    "blocking" variety:
        consumer waits if queue is empty
        producer waits if queue is full

    struct Fifo {
      ItemType items*
      size_t writepos;
      size_t readpos;
      size_t capacity;
    }

sparse array

tree
    The cost of transferring data between levels of the memory hierarchy
    (RAM-to-cache or disk-to-RAM) dominates the cost of actual computation for
    many problems. "Cache-oblivious" data structures offer performance
    guarantees without explicit knowledge of the block-size.

    "threaded" binary tree points unused pointers to higher nodes. (Knuth 322)
        => enables traversal without recursion!

binary
    balanced: (maxdepth - mindepth) <= 1

BST
    TODO: treap https://jvns.ca/blog/2017/09/09/data-structure--the-treap-/

    find(): O(lgn) (_If_ you can guarantee that the number of nodes ~halved each
                    iteration. This is why BST must be "balanced".)
    Other important properties:
        can obtain the smallest element by following all the left children
        can obtain the largest element by following all the right children

heap
    TODO: https://github.com/wincent/command-t/blob/master/ruby/command-t/heap.c

    BAD  for search. "heap is not a BST"
    GOOD for priority queue: Find the min (max) element in a set under insertion
                             and deletion.
    Maintains _partial_ order on the set of elements which is weaker than the
    sorted order (=> efficient to maintain) yet stronger than random order
    (=> fast lookup of min element).
    _compact_ => Represents binary trees without using any pointers. Stores data
                 as an array of keys.

priority queue (heap)
    max_key() or min_key() on a Dictionary enables it to serve as a priority queue.
    heap is a maximally efficient implementation of priority queue

bloom filter
    probabilistic data structure: NO or PROBABLY
    time- AND space-efficient
    applications:
        webbrowser: check potential spam URL without hitting the network
        cache (Akamai): store in cache only if requested at least once before
        database: avoid disk lookup of non-existent rows/columns.
    to insert an element: O(1)
      1. Imagine you have k distinct, _deterministic_ hash functions.
      2. For a given element, each hash function returns a different value
         (collisions are okay).
      3. Using the output of each hash function as an index into a bit-array,
         set array[i]=true for each index i.
      - Time-cost is O(1) because each insertion runs a constant number of
        hash functions and sets a constant number of array indices
    to check for membership: O(1)
      1. Run it through all of the same hash functions again! Because the hash
         functions are deterministic, the same input should return the same
         output.
      2. For each index (hash result), check if array[i] == true.
         - If each case is true, result is PROBABLY (element was probably
           inserted in the past).
         - If any case is false, result is NO (no false negatives).

splay tree
    uses rotations to move any accessed key to the root. Frequently-used or
    recently-accessed nodes thus sit near the top of the tree, allowing faster
    searches.

B-tree ("balanced")
    For data larger than main memory (~1M+ items).
    self-balancing tree
    generalization of a binary search tree: N children per node
    concept: collapse several levels of a binary search tree into a single large
             node, so we can do several search steps before another disk access
             is needed. Can access enormous numbers of keys using only a few
             disk accesses.

    TODO: http://patshaughnessy.net/2014/11/11/discovering-the-computer-science-behind-postgres-indexes

merkle tree
    TODO: https://blog.cloudflare.com/introducing-certificate-transparency-and-nimbus/
sparse merkle tree (SMT)
    - mentioned in "The Power of Toys" (David Nolen)
    - like a standard Merkle tree, except the contained data is indexed. missing data is stored as null.
    - pro: Efficient proofs of non-inclusion. Can check that data is *not* part of the tree (unlike normal MT)
    - con: use lots of space. (mitigation: 
    - represents a key-value store, inside of a Merkle tree.
    - Ethereum researchers are looking into sparse Merkle trees as a replacement for the Merkle Patricia tries currently used to store Ethereum state.

trie (ReTRIEval, "prefix trie", "suffix trie", …)
    examples: autocomplete, spellcheck, hierarchical routing (e.g. ip ranges)

graphs
    TODO: BFS can be used to test bipartiteness, by starting the search at any
          vertex and giving alternating labels to the vertices visited during
          the search. That is, give label 0 to the starting vertex, 1 to all its
          neighbors, 0 to those neighbors' neighbors, and so on. If at any step
          a vertex has (visited) neighbors with the same label as itself, then
          the graph is not bipartite.

    Adjacency list is usually the best data structure to use for graphs.
    Most real-world graphs are sparse.

    Adjacency Lists:
        space:          O(m + n)
        insert/delete:  O(d)
        traverse:       Θ(m + n)
    compact representation using linked lists.
    Harder (vs adj. matrix) to verify whether a given edge (i,j) is in G, since
    we must search through the list. However, such queries usually are not
    needed, instead BF/DF traversal is used and nodes are updated passively.

    Adjacency Matrix:
        space:          O(n^2)
        insert/delete:  O(1)
        traverse:       Θ(n^2)
    We can represent G using an n × n matrix M, where
    element M[i,j] = 1 if (i,j) is an edge of G, and 0 if it isn’t. This allows fast
    answers to the question “is (i,j) in G?”, and rapid updates for edge insertion
    and deletion. It may use excessive space for graphs with many vertices and
    relatively few edges, however.

    problem: isomorphism test: determine whether the topological structures of
             two graphs are identical if we ignore any labels.
    soln:    backtracking: assign each vertex in each graph a label, then
             compare.


MEMORY MODELS http://canonical.org/~kragen/memory-models/
=========================================================
COBOL: nested records
    - fixed-size fields
LISP: object graph (labeled, directed)
    - requires GC
    - Serializing an object graph is a bit tricky, both because it can contain circular references, but also because the part you want to serialize may contain references to a part you don’t want to serialize, and you have to special-case both of these. For example, in some systems, a class instance contains a reference to its class, and the class contains references not only to the current versions of all the methods but also to its superclass, but maybe you don’t want to serialize the entire bytecode for the class in every serialized object. 
    - Most popular programming languages use this model
FORTRAN: parallel arrays
    - parallel arrays are cache-friendly, support different visibility for different attributes, support setting watchpoints, provide a sequence that can be meaningful, and support multidimensional indexing (where an attribute is a property of a tuple of entities, rather than just one entity). I would add that they also allow you to write subroutines that abstract over attributes, since they reify each attribute at run-time: you can write a sum function or a covariance function that can be applied to arbitrary attributes.
    - Octave, Matlab, APL, J, K, PV-WAVE IDL, Lush, S, S-Plus, and R are all significant parallel-array-oriented programming languages; Numpy, Pandas, and OpenGL are parallel-array-oriented libraries; and as I explained above, Perl4, awk, and Tcl are to some extent parallel-dictionary-oriented.
    - Various features of modern hardware increase the pressure to use parallel arrays to get better performance: the increasing gap between CPU speed and memory speed, the SIMT architecture of GPUs, and the SIMD instructions that have been added to CPUs to increase the ratio of ALU silicon to control silicon.
MAGNETIC-TAPE: pipes ("flow-based programming")
    - This kind of append-only storage turns out to be entirely adequate for some algorithms; MapReduce is not far from operating in this fashion, but also the typical problem of tokenization with lex, for example, uses just such a minimal interface to its input.
    - Python’s iterators and generators, the C++ STL’s forward iterators, D’s forward ranges, and Golang’s channels are all examples of such pipes or channels, with their purely sequential data access.
MULTICS: directories (string-labeled tree with blob leaves)
SQL: relations (collection of mutable multivalued finite functions)


